theme(legend.position = "bottom")
Irrigation_Intensity
Drought_Intensity <- ggplot(
data = metadat_dpreds,
aes(Percent_control, pred)
) +
geom_hline(yintercept = 0) +
scale_color_viridis_d(name = "Ecosystem") +
geom_smooth(method = lm, formula = y ~ x) +
geom_jitter(aes(color = Ecosystem_type),
width = 0.15, size = 4) +
stat_poly_eq(formula = y ~ x,
aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~"))) +
xlab("Percent of MAP") +
ylab("predicted ln(RR)") +
theme(legend.position = "bottom")
DI_margH <- ggMarginal(Drought_Intensity,
type = "histogram",
margins = "x", size = 4
)
DI_margH
knitr::opts_chunk$set(echo = FALSE)
# Load packages
# raster needs to come before dplyr load so 'select' isn't masked; we use `require`
# because only need it if the geotiffs are available (and not on GitHub Actions)
require(raster)
# Normal package loads
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggExtra)
library(kableExtra)
library(forcats)
library(DT)
library(metafor)
library(PerformanceAnalytics)
library(MuMIn)
if(packageVersion("MuMIn") > "1.43.15") {
stop("This analysis won't work with newer versions of MuMIn; please install 1.43.15")
}
library(dmetar)
library(broom)
library(bookdown)
library(ggmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggspatial)
library(sf)
library(ggpmisc)
theme_set(theme_minimal() + theme(text = element_text(size = 16)))
# 'Pretty n' function to round a numeric value and print that # of digits
pn <- function(x, n) {
formatC(round(unlist(x), n),
digits = n, format = "f")
}
# 'Clean p value' function to pretty-print p value(s), specifically
pclean <- function(x, digits = 3, printP = TRUE) {
x <- as.vector(x)
ltstring <- paste0("< 0.", paste(rep("0", digits - 1),
collapse = ""), "1")
valstring <- ifelse(x < 10 ^ -digits,
ltstring, pn(x, digits))
if(printP) {
paste("P", ifelse(x < 10 ^ -digits,
valstring, paste("=", valstring)))
} else {
valstring
}
}
# Read in SRDB and filter out bad values using the Quality_flag column
srdb_raw <- read.csv("./rs/srdb-data.csv") %>% as_tibble()
# Read in new studies 2018-2021 that SRDB doesn't have yet
read_csv("./rs/Rs Studies 2018+ - Water Manipulation.csv",
col_types = "cdddcdcccdcccdddddddddddc"
) %>%
select(
Study_number, Record_number, Author, Study_midyear, Latitude, Ecosystem_type,
Manipulation, Manipulation_level, Meas_method, Soil_type,
Rs_annual, Rh_annual, Rs_growingseason, Soil_drainage, Elevation
) -> new_studies
srdb_raw %>%
select(
Study_number, Record_number, Author, Study_midyear, Latitude, Ecosystem_type,
Manipulation, Manipulation_level, Meas_method, Species, Soil_type,
Rs_annual, Rh_annual, Rs_growingseason, Soil_drainage, Soil_clay, Elevation, Quality_flag
) %>%
filter(!grepl("Q1[0-5]", Quality_flag)) %>%
bind_rows(new_studies) -> srdb
read_csv("rs/Variance and N - Water manipulations.csv",
skip = 2,
col_types = "ddcdccccddcccdddddddddddc"
) %>%
filter(Study_number != 6066) %>%
select(
Study_number, Record_number, N, OtherFactor, Longitude, SD_Rs_annual, SD_Rh_annual,
SD_Rs_growingseason, Percent_control, SM_mean, SM_sd
) %>%
left_join(srdb, by = c("Study_number", "Record_number")) %>%
mutate(
SD_Rs_annual = SD_Rs_annual / 378.43, # converting from g per yr to Âµmol per s
SD_Rh_annual = SD_Rh_annual / 378.43, # this will make all final estimates comparable
Rs_annual = Rs_annual / 378.43,
Rh_annual = Rh_annual / 378.43,
Manipulation = case_when(
Percent_control < 100 ~ "Drought",
Percent_control > 100 ~ "Irrigation",
Percent_control == 100 ~ "Control",
TRUE ~ NA_character_
)
) -> dat_rs
install.packages("amerifluxr")
library(amerifluxr)
?ameriflux
?ameriflux
help(package = "ameriflux")
help(package = "amerifluxr")
install.packages("pander")
install.packages("data.table")
library(pander)
library(data.table)
sites <- amf_site_info()
sites_dt <- data.table::as.data.table(sites)
pander::pandoc.table(sites_dt[!is.na(DATA_START), .N, by = .(IGBP, DATA_POLICY)][order(IGBP)])
possible_ls <- sites_dt[IGBP %in% c("BSV", "CSH", "CVM",
"DBF", "DNF", "EBF",
"ENF", "GRA", "MF",
"OSH","SAV","WSA") &
!is.na(DATA_START) &
LOCATION_LAT < 50 &
DATA_POLICY == "CCBY4.0",
.(SITE_ID, SITE_NAME, DATA_START, DATA_END)]
data_aval <- data.table::as.data.table(amf_list_data(site_set = possible_ls$SITE_ID))
pander::pandoc.table(data_aval[c(1:10), ])
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
crols2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(crop_ls2)
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
crols2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(crop_ls2)
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = crop_ls2$SITE_ID,
var_set = c("FCH4", "SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4", "SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4", "SWC_2_1_1"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4", "SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "FCH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
View(data_aval_sub)
data_aval_sub <- data_aval[data_aval$BASENAME %in% "FCH4",
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "CH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "CH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
possible_ls <- sites_dt[IGBP %in% c("BSV", "CSH", "CVM",
"DBF", "DNF", "EBF",
"ENF", "GRA", "MF",
"OSH","SAV","WSA") &
!is.na(DATA_START) &
LOCATION_LAT < 60 &
DATA_POLICY == "CCBY4.0",
.(SITE_ID, SITE_NAME, DATA_START, DATA_END)]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID)]
View(data_aval_sub)
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean, Y2015 + Y2016 + Y2017 + Y2018),
by = .(SITE_ID)]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub2 <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID)]
options(datatable.optimize=1
)
options(datatable.optimize=1)
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub2 <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
data_aval_sub2 <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
ls2 <- data_aval_sub[(Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.50]
pander::pandoc.table(ls2)
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.20]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "CH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.20]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4","CH4","SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4","CH4"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
pander::pandoc.table(ls2)
update(R)
updateR()
install.packages(c("blob", "bookdown", "broom", "caret", "commonmark", "conquer", "crayon", "DEoptimR", "desc", "DT", "extrafont", "fansi", "geojsonsf", "gert", "ggExtra", "ggpmisc", "ggpp", "haven", "igraph", "kernlab", "knitr", "leafem", "leaflet", "lme4", "magrittr", "maptools", "mapview", "matrixStats", "MuMIn", "openssl", "parallelly", "plyr", "polynom", "processx", "psych", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "readxl", "rgdal", "rlang", "rmarkdown", "robustbase", "rprojroot", "RSQLite", "sass", "scales", "seriation", "sf", "skimr", "spatstat", "spatstat.core", "spatstat.data", "spatstat.geom", "spatstat.random", "spatstat.sparse", "testthat", "tinytex", "tmap", "tzdb", "uuid", "vctrs", "waldo", "webshot", "withr", "xfun", "zoo"))
install.packages(c("fansi", "kernlab", "lme4", "magrittr", "Rcpp", "rlang", "robustbase", "vctrs"))
install.packages(c("fansi", "kernlab", "lme4", "magrittr", "Rcpp", "rlang", "robustbase", "vctrs"))
?amf_download_big
?amf_download_bif
library(amerifluxr)
library(pander)
library(data.table)
?amf_download_bif
KM_meta <- amf_download_bif(
user_id = "kendalynnm",
user_email = "kendalynn.morris@pnnl.gov",
data_policy = "CCBY4.0",
agree_policy = TRUE,
intended_use = "synthesis",
intended_use_text = "methane fluxes across soil moisture contents",
verbose = TRUE,
out_dir = getwd()
)
getwd()
?system.file
rm(list=ls())
library(amerifluxr)
library(pander)
library(data.table)
getwd()
#check working directory, set if needed
setwd("methane/methane-synthesis/")
#check working directory, set if needed
#setwd("methane/methane-synthesis/")
library(amerifluxr)
library(pander)
library(data.table)
#read in the first site
BR_Npw <- amf_read_base(
file = "AMF_BR-Npw_BASE-BADM_1-5.zip",
unzip = TRUE,
parse_timestamp = TRUE)
View(BR_Npw)
#read in metadata
MD <- amf_read_bif(file = "AMF_AA-Net_BIF_CCBY4_20220228.xlsx")
#subset by target site
BR_Npw_MD <- MD[MD$SITE_ID == "BR_Npw",]
View(BR_Npw)
#subset by target site
BR_Npw_MD <- MD[MD$SITE_ID == "BR-Npw",]
View(BR_Npw_MD)
unique(MD$VARIABLE_GROUP)
unique(BR_Npw_MD$VARIABLE_GROUP)
View(BR_Npw_MD)
paper <- amf_extract_badm(bif_data = BR_Npw_MD,
select_group = "GRP_REFERENCE_PAPER")
paper
#read in the first site
Ho1 <- amf_read_base(
file = "AMF_US-Ho1_BASE-BADM_7-5.zip",
unzip = TRUE,
parse_timestamp = TRUE)
#subset by target site
Ho1_MD <- MD[MD$SITE_ID == "Ho1",]
unique(Ho1_MD$VARIABLE_GROUP)
View(Ho1)
View(Ho1)
#subset by target site
Ho1_MD <- MD[MD$SITE_ID == "US-Ho1",]
unique(Ho1_MD$VARIABLE_GROUP)
paper <- amf_extract_badm(bif_data = Ho1_MD,
select_group = "GRP_REFERENCE_PAPER")
paper
unique(Ho1_MD$VARIABLE_GROUP)
fluxes <- amf_extract_badm(bif_data = Ho1_MD,
select_group = "GRP_FLUX_MEASUREMENTS")
fluxes
soilchem <- amf_extract_badm(bif_data = Ho1_MD,
select_group = "GRP_SOIL_CHEM")
soilchem
#get data summary for 9 sites with methane fluxes
data_sum <- amf_summarize_data(site_set = data_aval_sub$SITE_ID,
var_set = c("FCH4", "SWC"))
sites <- amf_site_info()
sites_dt <- data.table::as.data.table(sites)
#all sites that are CCBY4.0 and not wetlands or agriculture
possible_ls <- sites_dt[IGBP %in% c("BSV", "CSH", "CVM",
"DBF", "DNF", "EBF",
"ENF", "GRA", "MF",
"OSH","SAV","WSA") &
!is.na(DATA_START) &
LOCATION_LAT < 60 &
DATA_POLICY == "CCBY4.0",
.(SITE_ID, SITE_NAME, DATA_START, DATA_END)]
data_aval <- data.table::as.data.table(amf_list_data(site_set = possible_ls$SITE_ID))
#now filter by those upland sites methane flux data
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4"),
.(SITE_ID, BASENAME)]
#get data summary for 9 sites with methane fluxes
data_sum <- amf_summarize_data(site_set = data_aval_sub$SITE_ID,
var_set = c("FCH4", "SWC"))
pander::pandoc.table(data_sum)
unique(Ho1_MD$VARIABLE_GROUP)
getwd()
library(dplyr)
library(tidyr)
library(readr)
library(amerifluxr)
View(results)
head(is.na(results$value))
is.na(results$value)
test <- results[is.na(results)]
test <- results[is.na(results),]
test
remove(test)
colnames(results)
unique(site) -> TS_check
results %>%
select(variable = "TS") %>%
unique(site) -> TS_check
results %>%
select(variable = TS) %>%
unique(site) -> TS_check
results %>%
select(variable = TS) %>%
unique(site) -> TS_check
results %>%
select(variable == TS) %>%
unique(site) -> TS_check
results %>%
select(variable == TS) %>%
unique(site) -> TS_check
results %>%
filter(variable == TS) %>%
unique(site) -> TS_check
results %>%
filter(variable == "TS") %>%
unique(site) -> TS_check
results %>%
filter(variable = "TS") %>%
unique(site) -> TS_check
results %>%
filter(variable = TS) %>%
unique(site) -> TS_check
results %>%
filter(variable == TS) %>%
unique(site) -> TS_check
results %>%
filter(variable == "TS") %>%
unique(site) -> TS_check
results %>%
subset(variable == "TS") %>%
unique(site) -> TS_check
results %>%
subset(variable == "TS") -> TS_check
unique(TS_check$sites)
View(TS_check)
unique(TS_check$site)
length(unique(TS_check$site))
View(results)
library(dplyr)
library(tidyr)
library(readr)
library(amerifluxr)
#get names of sites to process
sites <- c("BR-Npw", "US-Ho1", "US-Jo1", "US-JRn", "US-NC2", "US-PFa", "US-Sne", "US-Snf", "US-Tur")
#for loop: f in names of files
#TIMESTAMP start and end still coming through
#read in data
results <- list()
#for loop: f in names of files
#TIMESTAMP start and end still coming through
#read in data
results <- list()
tf <- "tempfile.csv"
if(file.exists(tf)) file.remove(tf)
for (site in sites) {
filename <- list.files(pattern = site)
message (site," Found ", length(filename), " files")
dat_raw <- amf_read_base(file = filename,
unzip = TRUE,
parse_timestamp = TRUE)
#columns to keep
ctk <- grep("^(TIMESTAMP$|CO2|CH4|FC|FCH4|PA|^P$|^P_|SWC|^TA$|^TA_|TS|WTD)", colnames(dat_raw))
message ("Data has ", ncol(dat_raw), " keeping ", length(ctk))
dat <- dat_raw[ctk]
# reshape data to long form and add site name
dat <- dat %>%
pivot_longer(-TIMESTAMP) %>%
separate(name, into = c("variable", "other"),
sep = "_", extra="merge", fill="right") %>%
drop_na(value)
dat$site <- site
write_csv(dat, file = tf, append = TRUE, col_names = !file.exists(tf))
}
results <- read.csv(tf)
results %>%
subset(variable == "TS") -> TS_check
length(unique(TS_check$site))
unique(TS_check$site)
unique(results$site)
View(results)
length(is.na(results$value == TRUE))
results[is.na(results$value),]
remove(TS_check)
results %>%
filter(variable == "TS", other == "_1_2_1") %>%
unique(site)
results %>%
filter(variable == "TS", other == "_1_2_1") %>%
unique(results$site)
