summary(metadat_dmodel)
dp <- tidy(metadat_dmodel)
ocs_dp <- dp$p.value[dp$term == "sg_ocs"]
profile(metadat_dmodel)
forest.rma(metadat_dmodel)
funnel(metadat_dmodel)
plot(cooks.distance(metadat_dmodel), type = "o")
dpreds <- predict.rma(metadat_dmodel)
#combine columns into one dataframe
metadat_d %>%
bind_cols(as.data.frame(dpreds)) -> metadat_dpreds
# Cook's Distance n/4 for drought studies
length(metadat[metadat$Manipulation == "Drought", ]$Manipulation) / 4
#One study (13079) has a strong influence
#4 years of data for grassland and desert
Coef_drought <- coef(summary(metadat_dmodel))
dp
clay_dp <- dp$p.value[dp$term == "sg_clay"]
clay_dp
ocs_d
ocs_dp
ocs_ip
#create of subset of metadat which only includes our model inputs
metadat %>%
filter(Manipulation == "Irrigation") %>%
select("Study_number", "Ecosystem_type",
"Duration", "sg_ocs", "yi",
"vi") %>%
na.omit() -> metadat_i
# rma.mv conducts a meta-analysis multivariate mixed effects model
# the - 1 for the fixed effect modifiers gives us estimates for individual ecosystem types
metadat_imodel <- rma.mv(yi, vi,
random = ~ 1 | Study_number,
mods = ~ Ecosystem_type * Duration + sg_ocs - 1,
method = "REML",
data = metadat_i,
slab = Ecosystem_type,
)
summary(metadat_imodel)
ip <- tidy(metadat_imodel)
desert_p <- ip$p.value[ip$term == "Ecosystem_typeDesert"]
ocs_ip <- ip$p.value[ip$term == "sg_ocs"]
ipreds <- predict.rma(metadat_imodel)
profile(metadat_imodel)
forest.rma(metadat_imodel)
funnel(metadat_imodel)
plot(cooks.distance(metadat_imodel), type = "o")
# Cook's Distance n/4 for irrigation studies
length(metadat[metadat$Manipulation == "Irrigation", ]$Manipulation) / 4
# Looks good!
Coef_irrigation <- coef(summary(metadat_imodel))
ocs_ip
ip
FD_ip <- ip$p.value[ip$term == "Ecosystem_typeForest:Duration"]
FD_ip
GD_ip <- ip$p.value[ip$term == "Ecosystem_typeGrassland:Duration"]
GD_ip
pclean(FD_ip)
pclean(DD_ip)
pclean(GD_ip)
library(ggpmisc)
Irrigation_Intensity <- ggplot(
data = metadat[metadat$Manipulation == "Irrigation",],
aes(Percent_control, yi)
) +
geom_hline(yintercept = 0) +
geom_jitter(na.rm = TRUE, aes(color = Ecosystem_type),
width = 0.5, size = 4) +
scale_color_viridis_d(name = "Ecosystem") +
geom_smooth(method = lm, formula = y ~ x) +
stat_poly_eq() +
ylab("ln(RR)") +
xlab("Percent of MAP") +
theme(legend.position = "bottom")
Irrigation_Intensity
Irrigation_Intensity <- ggplot(
Irrigation_Intensity <- ggplot(
data = metadat[metadat$Manipulation == "Irrigation",],
aes(Percent_control, yi)
) +
geom_hline(yintercept = 0) +
geom_jitter(na.rm = TRUE, aes(color = Ecosystem_type),
width = 0.5, size = 4) +
scale_color_viridis_d(name = "Ecosystem") +
geom_smooth(method = lm, formula = y ~ x) +
stat_poly_eq(formula = y ~ x,
aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~"))) +
ylab("ln(RR)") +
xlab("Percent of MAP") +
theme(legend.position = "bottom")
II_margH <- ggMarginal(Irrigation_Intensity,
type = "histogram",
margins = "x", size = 4
)
II_margH
Irrigation_Intensity <- ggplot(
data = metadat[metadat$Manipulation == "Irrigation",],
aes(Percent_control, yi)
) +
geom_hline(yintercept = 0) +
geom_jitter(na.rm = TRUE, aes(color = Ecosystem_type),
width = 0.5, size = 4) +
scale_color_viridis_d(name = "Ecosystem") +
geom_smooth(method = lm, formula = y ~ x) +
stat_poly_eq(formula = y ~ x,
aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~"))) +
ylab("ln(RR)") +
xlab("Percent of MAP") +
theme(legend.position = "bottom")
Irrigation_Intensity
Drought_Intensity <- ggplot(
data = metadat_dpreds,
aes(Percent_control, pred)
) +
geom_hline(yintercept = 0) +
scale_color_viridis_d(name = "Ecosystem") +
geom_smooth(method = lm, formula = y ~ x) +
geom_jitter(aes(color = Ecosystem_type),
width = 0.15, size = 4) +
stat_poly_eq(formula = y ~ x,
aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~"))) +
xlab("Percent of MAP") +
ylab("predicted ln(RR)") +
theme(legend.position = "bottom")
DI_margH <- ggMarginal(Drought_Intensity,
type = "histogram",
margins = "x", size = 4
)
DI_margH
knitr::opts_chunk$set(echo = FALSE)
# Load packages
# raster needs to come before dplyr load so 'select' isn't masked; we use `require`
# because only need it if the geotiffs are available (and not on GitHub Actions)
require(raster)
# Normal package loads
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggExtra)
library(kableExtra)
library(forcats)
library(DT)
library(metafor)
library(PerformanceAnalytics)
library(MuMIn)
if(packageVersion("MuMIn") > "1.43.15") {
stop("This analysis won't work with newer versions of MuMIn; please install 1.43.15")
}
library(dmetar)
library(broom)
library(bookdown)
library(ggmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggspatial)
library(sf)
library(ggpmisc)
theme_set(theme_minimal() + theme(text = element_text(size = 16)))
# 'Pretty n' function to round a numeric value and print that # of digits
pn <- function(x, n) {
formatC(round(unlist(x), n),
digits = n, format = "f")
}
# 'Clean p value' function to pretty-print p value(s), specifically
pclean <- function(x, digits = 3, printP = TRUE) {
x <- as.vector(x)
ltstring <- paste0("< 0.", paste(rep("0", digits - 1),
collapse = ""), "1")
valstring <- ifelse(x < 10 ^ -digits,
ltstring, pn(x, digits))
if(printP) {
paste("P", ifelse(x < 10 ^ -digits,
valstring, paste("=", valstring)))
} else {
valstring
}
}
# Read in SRDB and filter out bad values using the Quality_flag column
srdb_raw <- read.csv("./rs/srdb-data.csv") %>% as_tibble()
# Read in new studies 2018-2021 that SRDB doesn't have yet
read_csv("./rs/Rs Studies 2018+ - Water Manipulation.csv",
col_types = "cdddcdcccdcccdddddddddddc"
) %>%
select(
Study_number, Record_number, Author, Study_midyear, Latitude, Ecosystem_type,
Manipulation, Manipulation_level, Meas_method, Soil_type,
Rs_annual, Rh_annual, Rs_growingseason, Soil_drainage, Elevation
) -> new_studies
srdb_raw %>%
select(
Study_number, Record_number, Author, Study_midyear, Latitude, Ecosystem_type,
Manipulation, Manipulation_level, Meas_method, Species, Soil_type,
Rs_annual, Rh_annual, Rs_growingseason, Soil_drainage, Soil_clay, Elevation, Quality_flag
) %>%
filter(!grepl("Q1[0-5]", Quality_flag)) %>%
bind_rows(new_studies) -> srdb
read_csv("rs/Variance and N - Water manipulations.csv",
skip = 2,
col_types = "ddcdccccddcccdddddddddddc"
) %>%
filter(Study_number != 6066) %>%
select(
Study_number, Record_number, N, OtherFactor, Longitude, SD_Rs_annual, SD_Rh_annual,
SD_Rs_growingseason, Percent_control, SM_mean, SM_sd
) %>%
left_join(srdb, by = c("Study_number", "Record_number")) %>%
mutate(
SD_Rs_annual = SD_Rs_annual / 378.43, # converting from g per yr to Âµmol per s
SD_Rh_annual = SD_Rh_annual / 378.43, # this will make all final estimates comparable
Rs_annual = Rs_annual / 378.43,
Rh_annual = Rh_annual / 378.43,
Manipulation = case_when(
Percent_control < 100 ~ "Drought",
Percent_control > 100 ~ "Irrigation",
Percent_control == 100 ~ "Control",
TRUE ~ NA_character_
)
) -> dat_rs
install.packages("amerifluxr")
library(amerifluxr)
?ameriflux
?ameriflux
help(package = "ameriflux")
help(package = "amerifluxr")
install.packages("pander")
install.packages("data.table")
library(pander)
library(data.table)
sites <- amf_site_info()
sites_dt <- data.table::as.data.table(sites)
pander::pandoc.table(sites_dt[!is.na(DATA_START), .N, by = .(IGBP, DATA_POLICY)][order(IGBP)])
possible_ls <- sites_dt[IGBP %in% c("BSV", "CSH", "CVM",
"DBF", "DNF", "EBF",
"ENF", "GRA", "MF",
"OSH","SAV","WSA") &
!is.na(DATA_START) &
LOCATION_LAT < 50 &
DATA_POLICY == "CCBY4.0",
.(SITE_ID, SITE_NAME, DATA_START, DATA_END)]
data_aval <- data.table::as.data.table(amf_list_data(site_set = possible_ls$SITE_ID))
pander::pandoc.table(data_aval[c(1:10), ])
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
crols2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(crop_ls2)
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
crols2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(crop_ls2)
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = crop_ls2$SITE_ID,
var_set = c("FCH4", "SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4", "SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4", "SWC_2_1_1"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4", "SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "FCH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
View(data_aval_sub)
data_aval_sub <- data_aval[data_aval$BASENAME %in% "FCH4",
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "CH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "CH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
possible_ls <- sites_dt[IGBP %in% c("BSV", "CSH", "CVM",
"DBF", "DNF", "EBF",
"ENF", "GRA", "MF",
"OSH","SAV","WSA") &
!is.na(DATA_START) &
LOCATION_LAT < 60 &
DATA_POLICY == "CCBY4.0",
.(SITE_ID, SITE_NAME, DATA_START, DATA_END)]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID)]
View(data_aval_sub)
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean, Y2015 + Y2016 + Y2017 + Y2018),
by = .(SITE_ID)]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub2 <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID)]
options(datatable.optimize=1
)
options(datatable.optimize=1)
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub2 <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
data_aval_sub2 <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub2<- data_aval_sub2[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
ls2 <- data_aval_sub[(Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.75]
pander::pandoc.table(ls2)
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.50]
pander::pandoc.table(ls2)
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.20]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = "CH4",
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4","CH4","SWC"),
.(SITE_ID, BASENAME, Y2015, Y2016, Y2017, Y2018)]
data_aval_sub <- data_aval_sub[, lapply(.SD, mean),
by = .(SITE_ID),
.SDcols = c("Y2015", "Y2016", "Y2017", "Y2018")]
ls2 <- data_aval_sub[(Y2015 + Y2016 + Y2017 + Y2018) / 4 > 0.20]
pander::pandoc.table(ls2)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4","CH4","SWC"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
amf_plot_datayear(
site_set = ls2$SITE_ID,
var_set = c("FCH4","CH4"),
nonfilled_only = TRUE,
year_set = c(2015:2018)
)
pander::pandoc.table(ls2)
update(R)
updateR()
install.packages(c("blob", "bookdown", "broom", "caret", "commonmark", "conquer", "crayon", "DEoptimR", "desc", "DT", "extrafont", "fansi", "geojsonsf", "gert", "ggExtra", "ggpmisc", "ggpp", "haven", "igraph", "kernlab", "knitr", "leafem", "leaflet", "lme4", "magrittr", "maptools", "mapview", "matrixStats", "MuMIn", "openssl", "parallelly", "plyr", "polynom", "processx", "psych", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "readxl", "rgdal", "rlang", "rmarkdown", "robustbase", "rprojroot", "RSQLite", "sass", "scales", "seriation", "sf", "skimr", "spatstat", "spatstat.core", "spatstat.data", "spatstat.geom", "spatstat.random", "spatstat.sparse", "testthat", "tinytex", "tmap", "tzdb", "uuid", "vctrs", "waldo", "webshot", "withr", "xfun", "zoo"))
install.packages(c("fansi", "kernlab", "lme4", "magrittr", "Rcpp", "rlang", "robustbase", "vctrs"))
install.packages(c("fansi", "kernlab", "lme4", "magrittr", "Rcpp", "rlang", "robustbase", "vctrs"))
?amf_download_big
?amf_download_bif
library(amerifluxr)
library(pander)
library(data.table)
?amf_download_bif
KM_meta <- amf_download_bif(
user_id = "kendalynnm",
user_email = "kendalynn.morris@pnnl.gov",
data_policy = "CCBY4.0",
agree_policy = TRUE,
intended_use = "synthesis",
intended_use_text = "methane fluxes across soil moisture contents",
verbose = TRUE,
out_dir = getwd()
)
getwd()
?system.file
rm(list=ls())
library(amerifluxr)
library(pander)
library(data.table)
getwd()
#check working directory, set if needed
setwd("methane/methane-synthesis/")
#check working directory, set if needed
#setwd("methane/methane-synthesis/")
library(amerifluxr)
library(pander)
library(data.table)
#read in the first site
BR_Npw <- amf_read_base(
file = "AMF_BR-Npw_BASE-BADM_1-5.zip",
unzip = TRUE,
parse_timestamp = TRUE)
View(BR_Npw)
#read in metadata
MD <- amf_read_bif(file = "AMF_AA-Net_BIF_CCBY4_20220228.xlsx")
#subset by target site
BR_Npw_MD <- MD[MD$SITE_ID == "BR_Npw",]
View(BR_Npw)
#subset by target site
BR_Npw_MD <- MD[MD$SITE_ID == "BR-Npw",]
View(BR_Npw_MD)
unique(MD$VARIABLE_GROUP)
unique(BR_Npw_MD$VARIABLE_GROUP)
View(BR_Npw_MD)
paper <- amf_extract_badm(bif_data = BR_Npw_MD,
select_group = "GRP_REFERENCE_PAPER")
paper
#read in the first site
Ho1 <- amf_read_base(
file = "AMF_US-Ho1_BASE-BADM_7-5.zip",
unzip = TRUE,
parse_timestamp = TRUE)
#subset by target site
Ho1_MD <- MD[MD$SITE_ID == "Ho1",]
unique(Ho1_MD$VARIABLE_GROUP)
View(Ho1)
View(Ho1)
#subset by target site
Ho1_MD <- MD[MD$SITE_ID == "US-Ho1",]
unique(Ho1_MD$VARIABLE_GROUP)
paper <- amf_extract_badm(bif_data = Ho1_MD,
select_group = "GRP_REFERENCE_PAPER")
paper
unique(Ho1_MD$VARIABLE_GROUP)
fluxes <- amf_extract_badm(bif_data = Ho1_MD,
select_group = "GRP_FLUX_MEASUREMENTS")
fluxes
soilchem <- amf_extract_badm(bif_data = Ho1_MD,
select_group = "GRP_SOIL_CHEM")
soilchem
#get data summary for 9 sites with methane fluxes
data_sum <- amf_summarize_data(site_set = data_aval_sub$SITE_ID,
var_set = c("FCH4", "SWC"))
sites <- amf_site_info()
sites_dt <- data.table::as.data.table(sites)
#all sites that are CCBY4.0 and not wetlands or agriculture
possible_ls <- sites_dt[IGBP %in% c("BSV", "CSH", "CVM",
"DBF", "DNF", "EBF",
"ENF", "GRA", "MF",
"OSH","SAV","WSA") &
!is.na(DATA_START) &
LOCATION_LAT < 60 &
DATA_POLICY == "CCBY4.0",
.(SITE_ID, SITE_NAME, DATA_START, DATA_END)]
data_aval <- data.table::as.data.table(amf_list_data(site_set = possible_ls$SITE_ID))
#now filter by those upland sites methane flux data
data_aval_sub <- data_aval[data_aval$BASENAME %in% c("FCH4"),
.(SITE_ID, BASENAME)]
#get data summary for 9 sites with methane fluxes
data_sum <- amf_summarize_data(site_set = data_aval_sub$SITE_ID,
var_set = c("FCH4", "SWC"))
pander::pandoc.table(data_sum)
unique(Ho1_MD$VARIABLE_GROUP)
